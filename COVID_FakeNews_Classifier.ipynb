{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Fake News Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Imports y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, json, os, joblib\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "STOP = set(stopwords.words('english'))\n",
    "\n",
    "# funciones auxiliares\n",
    "def clean_text(s:str)->str:\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r\"http\\S+|www\\S+\",\" \",s)\n",
    "    s = re.sub(r\"[^a-zA-Z\\s]\",\" \",s)\n",
    "    s = re.sub(r\"\\s+\",\" \",s).strip()\n",
    "    s = \" \".join([w for w in s.split() if w not in STOP])\n",
    "    return s\n",
    "\n",
    "\n",
    "def binarize_label(sentiment:str)->int:\n",
    "    neg = {\"negative\", \"extremely negative\"}\n",
    "    s = str(sentiment).strip().lower()\n",
    "    return 0 if s in neg else 1  # Fake=0, Real=1\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306172d4",
   "metadata": {},
   "source": [
    "## 2) Carga de datos y preparación de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (41157, 6)\n",
      "Test shape: (3798, 6)\n",
      "\n",
      "Columnas: ['UserName', 'ScreenName', 'Location', 'TweetAt', 'OriginalTweet', 'Sentiment']\n",
      "\n",
      "Ejemplo texto limpio:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>menyrbie phil gahan chrisitv</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>advice talk neighbours family exchange phone n...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>coronavirus australia woolworths give elderly ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       OriginalTweet  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...   \n",
       "1  advice Talk to your neighbours family to excha...   \n",
       "2  Coronavirus Australia: Woolworths to give elde...   \n",
       "\n",
       "                                          clean_text Sentiment  label  \n",
       "0                       menyrbie phil gahan chrisitv   Neutral      1  \n",
       "1  advice talk neighbours family exchange phone n...  Positive      1  \n",
       "2  coronavirus australia woolworths give elderly ...  Positive      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Asegúrate de tener los archivos en el mismo directorio de trabajo del notebook\n",
    "train_path = \"Corona_NLP_train.csv\"\n",
    "test_path  = \"Corona_NLP_test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path, encoding=\"latin-1\")\n",
    "test_df  = pd.read_csv(test_path,  encoding=\"latin-1\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"\\nColumnas:\", list(train_df.columns))\n",
    "\n",
    "# Columnas esperadas: 'OriginalTweet' y 'Sentiment'\n",
    "train_df['label'] = train_df['Sentiment'].apply(binarize_label)\n",
    "test_df['label']  = test_df['Sentiment'].apply(binarize_label)\n",
    "\n",
    "train_df['clean_text'] = train_df['OriginalTweet'].apply(clean_text)\n",
    "test_df['clean_text']  = test_df['OriginalTweet'].apply(clean_text)\n",
    "\n",
    "X_train, y_train = train_df['clean_text'], train_df['label']\n",
    "X_test,  y_test  = test_df['clean_text'],  test_df['label']\n",
    "\n",
    "print(\"\\nEjemplo texto limpio:\")\n",
    "display(train_df[['OriginalTweet','clean_text','Sentiment','label']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Pipeline 1 — TF-IDF + modelos clásicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LogisticRegression ---\n",
      "Accuracy: 0.8009478672985783  | F1: 0.8388059701492537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8445    0.6583    0.7398      1633\n",
      "           1     0.7790    0.9085    0.8388      2165\n",
      "\n",
      "    accuracy                         0.8009      3798\n",
      "   macro avg     0.8117    0.7834    0.7893      3798\n",
      "weighted avg     0.8072    0.8009    0.7963      3798\n",
      "\n",
      "\n",
      "--- LinearSVC ---\n",
      "Accuracy: 0.8212216956292786  | F1: 0.8473814340301191\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8151    0.7557    0.7842      1633\n",
      "           1     0.8253    0.8707    0.8474      2165\n",
      "\n",
      "    accuracy                         0.8212      3798\n",
      "   macro avg     0.8202    0.8132    0.8158      3798\n",
      "weighted avg     0.8209    0.8212    0.8202      3798\n",
      "\n",
      "\n",
      "--- RandomForest ---\n",
      "Accuracy: 0.7669826224328594  | F1: 0.8113408654871029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7940    0.6185    0.6954      1633\n",
      "           1     0.7534    0.8790    0.8113      2165\n",
      "\n",
      "    accuracy                         0.7670      3798\n",
      "   macro avg     0.7737    0.7487    0.7533      3798\n",
      "weighted avg     0.7708    0.7670    0.7615      3798\n",
      "\n",
      "\n",
      "[TF-IDF] Mejor modelo: LinearSVC\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=50000, ngram_range=(1,2))\n",
    "Xtr_tfidf = tfidf.fit_transform(X_train)\n",
    "Xte_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Modelos clásicos\n",
    "models_tfidf = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=2000, random_state=RANDOM_STATE),\n",
    "    \"LinearSVC\": LinearSVC(random_state=RANDOM_STATE),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "}\n",
    "\n",
    "results_tfidf = {}\n",
    "for name, mdl in models_tfidf.items():\n",
    "    mdl.fit(Xtr_tfidf, y_train)\n",
    "    pred = mdl.predict(Xte_tfidf)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    f1  = f1_score(y_test, pred)\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(\"Accuracy:\", acc, \" | F1:\", f1)\n",
    "    print(classification_report(y_test, pred, digits=4))\n",
    "    results_tfidf[name] = {\"acc\": float(acc), \"f1\": float(f1), \"model\": mdl}\n",
    "\n",
    "# Guardamos el mejor (por F1) de TF-IDF\n",
    "best_tfidf_name = sorted(results_tfidf.keys(), key=lambda k: results_tfidf[k][\"f1\"], reverse=True)[0]\n",
    "best_tfidf_model = results_tfidf[best_tfidf_name][\"model\"]\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.joblib\")\n",
    "joblib.dump(best_tfidf_model, f\"tfidf_{best_tfidf_name}.joblib\")\n",
    "print(f\"\\n[TF-IDF] Mejor modelo: {best_tfidf_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Pipeline 2 — Word2Vec + modelos clásicos (sin Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LogisticRegression ---\n",
      "Accuracy: 0.6750921537651395  | F1: 0.7554498612762585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7176    0.4029    0.5161      1633\n",
      "           1     0.6616    0.8804    0.7554      2165\n",
      "\n",
      "    accuracy                         0.6751      3798\n",
      "   macro avg     0.6896    0.6417    0.6358      3798\n",
      "weighted avg     0.6856    0.6751    0.6525      3798\n",
      "\n",
      "\n",
      "--- LinearSVC ---\n",
      "Accuracy: 0.6827277514481306  | F1: 0.7612443035466614\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7336    0.4115    0.5273      1633\n",
      "           1     0.6666    0.8873    0.7612      2165\n",
      "\n",
      "    accuracy                         0.6827      3798\n",
      "   macro avg     0.7001    0.6494    0.6443      3798\n",
      "weighted avg     0.6954    0.6827    0.6606      3798\n",
      "\n",
      "\n",
      "--- RandomForest ---\n",
      "Accuracy: 0.6708794102159031  | F1: 0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6989    0.4121    0.5185      1633\n",
      "           1     0.6614    0.8661    0.7500      2165\n",
      "\n",
      "    accuracy                         0.6709      3798\n",
      "   macro avg     0.6801    0.6391    0.6342      3798\n",
      "weighted avg     0.6775    0.6709    0.6505      3798\n",
      "\n",
      "\n",
      "[W2V] Mejor modelo: LinearSVC\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenización simple (ya limpio)\n",
    "train_tokens = [t.split() for t in X_train]\n",
    "test_tokens  = [t.split() for t in X_test]\n",
    "\n",
    "# Entrenar Word2Vec\n",
    "w2v_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)\n",
    "dim = w2v_model.vector_size\n",
    "\n",
    "def doc_vector(tokens, model, dim):\n",
    "    vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    if not vecs:\n",
    "        return np.zeros(dim, dtype=np.float32)\n",
    "    return np.mean(vecs, axis=0).astype(np.float32)\n",
    "\n",
    "Xtr_w2v = np.vstack([doc_vector(t, w2v_model, dim) for t in train_tokens])\n",
    "Xte_w2v = np.vstack([doc_vector(t, w2v_model, dim) for t in test_tokens])\n",
    "\n",
    "models_w2v = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=2000, random_state=RANDOM_STATE),\n",
    "    \"LinearSVC\": LinearSVC(random_state=RANDOM_STATE),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "}\n",
    "\n",
    "results_w2v = {}\n",
    "for name, mdl in models_w2v.items():\n",
    "    mdl.fit(Xtr_w2v, y_train)\n",
    "    pred = mdl.predict(Xte_w2v)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    f1  = f1_score(y_test, pred)\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(\"Accuracy:\", acc, \" | F1:\", f1)\n",
    "    print(classification_report(y_test, pred, digits=4))\n",
    "    results_w2v[name] = {\"acc\": float(acc), \"f1\": float(f1), \"model\": mdl}\n",
    "\n",
    "# Guardar bundle Word2Vec + mejor clasificador (por F1)\n",
    "best_w2v_name = sorted(results_w2v.keys(), key=lambda k: results_w2v[k][\"f1\"], reverse=True)[0]\n",
    "best_w2v_model = results_w2v[best_w2v_name][\"model\"]\n",
    "joblib.dump({\"w2v\": w2v_model, \"clf\": best_w2v_model, \"dim\": dim}, \"w2v_best_bundle.joblib\")\n",
    "print(f\"\\n[W2V] Mejor modelo: {best_w2v_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Deep Learning — BiLSTM y CNN-1D (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive\\Desktop\\practicas\\AI\\chatgpt\\tf_env\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "547/547 - 206s - 377ms/step - accuracy: 0.7905 - loss: 0.4476 - val_accuracy: 0.8562 - val_loss: 0.3511\n",
      "Epoch 2/10\n",
      "547/547 - 177s - 323ms/step - accuracy: 0.9192 - loss: 0.2150 - val_accuracy: 0.8592 - val_loss: 0.3479\n",
      "Epoch 3/10\n",
      "547/547 - 185s - 338ms/step - accuracy: 0.9581 - loss: 0.1198 - val_accuracy: 0.8541 - val_loss: 0.4119\n",
      "Epoch 4/10\n",
      "547/547 - 209s - 382ms/step - accuracy: 0.9770 - loss: 0.0697 - val_accuracy: 0.8419 - val_loss: 0.5188\n",
      "Epoch 5/10\n",
      "547/547 - 213s - 389ms/step - accuracy: 0.9854 - loss: 0.0447 - val_accuracy: 0.8448 - val_loss: 0.6120\n",
      "BiLSTM test acc: 0.8288573026657104\n",
      "Epoch 1/10\n",
      "547/547 - 84s - 153ms/step - accuracy: 0.7848 - loss: 0.4532 - val_accuracy: 0.8699 - val_loss: 0.3252\n",
      "Epoch 2/10\n",
      "547/547 - 83s - 152ms/step - accuracy: 0.9110 - loss: 0.2327 - val_accuracy: 0.8654 - val_loss: 0.3284\n",
      "Epoch 3/10\n",
      "547/547 - 77s - 141ms/step - accuracy: 0.9636 - loss: 0.1023 - val_accuracy: 0.8531 - val_loss: 0.4171\n",
      "Epoch 4/10\n",
      "547/547 - 96s - 175ms/step - accuracy: 0.9867 - loss: 0.0434 - val_accuracy: 0.8447 - val_loss: 0.5445\n",
      "CNN-1D test acc: 0.8246445655822754\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Si no tienes TensorFlow instalado, instala y reinicia el kernel.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_words = 50000\n",
    "max_len   = 300\n",
    "\n",
    "tok = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tok.fit_on_texts(list(X_train))\n",
    "\n",
    "Xtr_seq = pad_sequences(tok.texts_to_sequences(X_train), maxlen=max_len, padding='post')\n",
    "Xte_seq = pad_sequences(tok.texts_to_sequences(X_test),  maxlen=max_len, padding='post')\n",
    "ytr = np.array(y_train)\n",
    "yte = np.array(y_test)\n",
    "\n",
    "joblib.dump({\"tokenizer\": tok, \"max_len\": max_len}, \"models_tokenizer.joblib\")\n",
    "\n",
    "def build_bilstm():\n",
    "    m = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "def build_cnn():\n",
    "    m = Sequential([\n",
    "        Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
    "        Conv1D(128, kernel_size=5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    m.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return m\n",
    "\n",
    "# BiLSTM\n",
    "bilstm = build_bilstm()\n",
    "cb1 = [EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),\n",
    "       ModelCheckpoint(\"bilstm_best.keras\", monitor='val_accuracy', save_best_only=True)]\n",
    "history_bilstm = bilstm.fit(Xtr_seq, ytr, validation_split=0.15, epochs=10, batch_size=64, callbacks=cb1, verbose=2)\n",
    "bilstm_acc = float(bilstm.evaluate(Xte_seq, yte, verbose=0)[1])\n",
    "print(\"BiLSTM test acc:\", bilstm_acc)\n",
    "\n",
    "# CNN-1D\n",
    "cnn = build_cnn()\n",
    "cb2 = [EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True),\n",
    "       ModelCheckpoint(\"cnn1d_best.keras\", monitor='val_accuracy', save_best_only=True)]\n",
    "history_cnn = cnn.fit(Xtr_seq, ytr, validation_split=0.15, epochs=10, batch_size=64, callbacks=cb2, verbose=2)\n",
    "cnn_acc = float(cnn.evaluate(Xte_seq, yte, verbose=0)[1])\n",
    "print(\"CNN-1D test acc:\", cnn_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1) Métricas F1 para modelos de DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bilstm_acc': 0.8288573026657104, 'bilstm_f1': 0.8505059797608095, 'cnn_acc': 0.8246445655822754, 'cnn_f1': 0.8552803129074316}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Obtener F1 para comparación justa\n",
    "pred_bilstm_proba = bilstm.predict(Xte_seq, verbose=0).ravel()\n",
    "pred_cnn_proba    = cnn.predict(Xte_seq, verbose=0).ravel()\n",
    "\n",
    "pred_bilstm = (pred_bilstm_proba >= 0.5).astype(int)\n",
    "pred_cnn    = (pred_cnn_proba    >= 0.5).astype(int)\n",
    "\n",
    "bilstm_f1 = f1_score(y_test, pred_bilstm)\n",
    "cnn_f1    = f1_score(y_test, pred_cnn)\n",
    "\n",
    "print({\"bilstm_acc\": bilstm_acc, \"bilstm_f1\": float(bilstm_f1),\n",
    "       \"cnn_acc\": cnn_acc, \"cnn_f1\": float(cnn_f1)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Selección y guardado del **mejor modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'cnn1d',\n",
       " 'type': 'dl',\n",
       " 'path': 'cnn1d_best.keras',\n",
       " 'f1': 0.8552803129074316,\n",
       " 'acc': 0.8246445655822754}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "candidates = []\n",
    "\n",
    "# TF-IDF best\n",
    "from glob import glob\n",
    "tfidf_model_paths = glob(\"tfidf_*.joblib\")\n",
    "if tfidf_model_paths:\n",
    "    # Re-eval para extraer F1 en test\n",
    "    best_path = tfidf_model_paths[0]\n",
    "    best_name = os.path.splitext(os.path.basename(best_path))[0].replace(\"tfidf_\",\"\")\n",
    "    tfidf_vec = joblib.load(\"tfidf_vectorizer.joblib\")\n",
    "    mdl = joblib.load(best_path)\n",
    "    yhat = mdl.predict(Xte_tfidf)\n",
    "    f1 = f1_score(y_test, yhat)\n",
    "    candidates.append({\"name\": f\"tfidf_{best_name}\", \"type\":\"tfidf\", \"path\": best_path,\n",
    "                       \"vec\":\"tfidf_vectorizer.joblib\", \"f1\": float(f1), \"acc\": float(accuracy_score(y_test, yhat))})\n",
    "\n",
    "# W2V best\n",
    "if os.path.exists(\"w2v_best_bundle.joblib\"):\n",
    "    w2v_bundle = joblib.load(\"w2v_best_bundle.joblib\")\n",
    "    clf = w2v_bundle[\"clf\"]\n",
    "    yhat = clf.predict(Xte_w2v)\n",
    "    f1 = f1_score(y_test, yhat)\n",
    "    candidates.append({\"name\":\"w2v_best\", \"type\":\"w2v\", \"path\":\"w2v_best_bundle.joblib\",\n",
    "                       \"f1\": float(f1), \"acc\": float(accuracy_score(y_test, yhat))})\n",
    "\n",
    "# DL\n",
    "candidates.append({\"name\":\"bilstm\", \"type\":\"dl\", \"path\":\"bilstm_best.keras\",\n",
    "                   \"f1\": float(bilstm_f1), \"acc\": float(bilstm_acc)})\n",
    "candidates.append({\"name\":\"cnn1d\", \"type\":\"dl\", \"path\":\"cnn1d_best.keras\",\n",
    "                   \"f1\": float(cnn_f1), \"acc\": float(cnn_acc)})\n",
    "\n",
    "# Elegimos por F1 y en empate por acc\n",
    "best = sorted(candidates, key=lambda x: (x.get(\"f1\",0.0), x.get(\"acc\",0.0)), reverse=True)[0]\n",
    "with open(\"best_model.json\",\"w\") as f:\n",
    "    json.dump(best, f, indent=2)\n",
    "best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) BONUS — Fine-tuning con DistilBERT (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\franc\\OneDrive\\Desktop\\practicas\\AI\\chatgpt\\tf_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 41157/41157 [00:17<00:00, 2396.93 examples/s]\n",
      "Map: 100%|██████████| 3798/3798 [00:00<00:00, 4912.45 examples/s]\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT no ejecutado (instala transformers/datasets/torch si lo necesitas). Error: ('Connection broken: IncompleteRead(267238885 bytes read, 715883 more expected)', IncompleteRead(267238885 bytes read, 715883 more expected))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    from datasets import Dataset\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "    import numpy as np\n",
    "\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize(batch):\n",
    "        return tok(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "    dtrain = Dataset.from_dict({\"text\": list(X_train), \"label\": list(y_train.astype(int))}).map(tokenize, batched=True)\n",
    "    dtest  = Dataset.from_dict({\"text\": list(X_test),  \"label\": list(y_test.astype(int))}).map(tokenize, batched=True)\n",
    "    dtrain.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "    dtest.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"label\"])\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return {\"accuracy\": accuracy_score(labels, preds), \"f1\": f1_score(labels, preds)}\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"bert_ckpt\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=2,            # sube a 3–4 si tienes GPU/tiempo\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        logging_steps=50\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=dtrain, eval_dataset=dtest, compute_metrics=compute_metrics)\n",
    "    trainer.train()\n",
    "    bert_metrics = trainer.evaluate(dtest)\n",
    "    print(\"DistilBERT metrics:\", bert_metrics)\n",
    "\n",
    "    trainer.save_model(\"models_bert_best\")\n",
    "    tok.save_pretrained(\"models_bert_best\")\n",
    "\n",
    "    # Añadir BERT a candidatos y, si gana, actualizar best_model.json\n",
    "    candidates_bert = json.load(open(\"best_model.json\"))\n",
    "    # Reabrimos todo para comparar fácil\n",
    "    all_cands = [candidates_bert] if isinstance(candidates_bert, dict) else []\n",
    "    all_cands.append({\"name\":\"distilbert\", \"type\":\"bert\", \"path\":\"models_bert_best\",\n",
    "                      \"f1\": float(bert_metrics.get(\"eval_f1\", 0.0)), \"acc\": float(bert_metrics.get(\"eval_accuracy\", 0.0))})\n",
    "    # Aquí no sabemos los demás candidatos si no los rearmamos; así que comparamos solo con el actual best.\n",
    "    best_now = sorted(all_cands, key=lambda x: (x.get(\"f1\",0.0), x.get(\"acc\",0.0)), reverse=True)[0]\n",
    "    with open(\"best_model.json\",\"w\") as f:\n",
    "        json.dump(best_now, f, indent=2)\n",
    "    print(\"Best (pos-BERT):\", best_now)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"DistilBERT no ejecutado (instala transformers/datasets/torch si lo necesitas). Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Exportar app web (Streamlit) desde el notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de app web escrito como: app_covid_news.py\n",
      "Ejecuta:  streamlit run app_covid_news.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "app_code = r'''\n",
    "import streamlit as st\n",
    "import json, joblib, numpy as np, re, os\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "STOP = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(s:str)->str:\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http\\S+|www\\S+\",\" \",s)\n",
    "    s = re.sub(r\"[^a-zA-Z\\s]\",\" \",s)\n",
    "    s = re.sub(r\"\\s+\",\" \",s).strip()\n",
    "    s = \" \".join([w for w in s.split() if w not in STOP])\n",
    "    return s\n",
    "\n",
    "st.title(\"📰 COVID-19 Fake News Classifier\")\n",
    "\n",
    "if not os.path.exists(\"best_model.json\"):\n",
    "    st.error(\"No se encontró 'best_model.json'. Ejecuta el notebook para entrenar y seleccionar el mejor modelo.\")\n",
    "    st.stop()\n",
    "\n",
    "with open(\"best_model.json\") as f:\n",
    "    BEST = json.load(f)\n",
    "\n",
    "txt = st.text_area(\"Pega una noticia/artículo:\")\n",
    "\n",
    "if st.button(\"Clasificar\"):\n",
    "    if not txt.strip():\n",
    "        st.warning(\"Escribe un texto.\")\n",
    "    else:\n",
    "        kind = BEST.get(\"type\")\n",
    "        if kind == \"tfidf\":\n",
    "            vec = joblib.load(BEST[\"vec\"])\n",
    "            model = joblib.load(BEST[\"path\"])\n",
    "            x = vec.transform([clean_text(txt)])\n",
    "            y = int(model.predict(x)[0])\n",
    "            proba = getattr(model, \"predict_proba\", None)\n",
    "            p = float(max(proba(x)[0])) if proba else None\n",
    "\n",
    "        elif kind == \"w2v\":\n",
    "            bundle = joblib.load(BEST[\"path\"])   # {\"w2v\":..., \"clf\":..., \"dim\":...}\n",
    "            tokens = [w for w in word_tokenize(clean_text(txt)) if w.isalpha()]\n",
    "            w2v, dim, clf = bundle[\"w2v\"], bundle[\"dim\"], bundle[\"clf\"]\n",
    "            vecs = [w2v.wv[w] for w in tokens if w in w2v.wv]\n",
    "            doc = np.mean(vecs, axis=0) if vecs else np.zeros(dim)\n",
    "            y = int(clf.predict([doc])[0]); p = None\n",
    "\n",
    "        elif kind == \"dl\":\n",
    "            from tensorflow.keras.models import load_model\n",
    "            info = joblib.load(\"models_tokenizer.joblib\")\n",
    "            tok, max_len = info[\"tokenizer\"], info[\"max_len\"]\n",
    "            from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "            seq = pad_sequences(tok.texts_to_sequences([txt]), maxlen=max_len, padding='post')\n",
    "            model = load_model(BEST[\"path\"])\n",
    "            prob = float(model.predict(seq, verbose=0)[0][0])\n",
    "            y = int(prob >= 0.5); p = prob if y==1 else 1-prob\n",
    "\n",
    "        elif kind == \"bert\":\n",
    "            from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "            import torch\n",
    "            name_or_path = BEST[\"path\"]\n",
    "            tok = AutoTokenizer.from_pretrained(name_or_path)\n",
    "            mdl = AutoModelForSequenceClassification.from_pretrained(name_or_path)\n",
    "            enc = tok([txt], truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                logits = mdl(**enc).logits\n",
    "                prob = torch.softmax(logits, dim=-1).numpy()[0]\n",
    "            y = int(np.argmax(prob))\n",
    "            p = float(prob[y])\n",
    "\n",
    "        else:\n",
    "            st.error(f\"Tipo de modelo no soportado: {kind}\")\n",
    "            st.stop()\n",
    "\n",
    "        label = \"REAL ✅\" if y==1 else \"FALSA ❌\"\n",
    "        st.success(f\"Predicción: {label}\" + (f\" | Confianza: {p:.2f}\" if p is not None else \"\"))\n",
    "'''\n",
    "\n",
    "with open(\"app_covid_news.py\",\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"Archivo de app web escrito como: app_covid_news.py\")\n",
    "print(\"Ejecuta:  streamlit run app_covid_news.py\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
